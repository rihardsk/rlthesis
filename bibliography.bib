% This file was created with JabRef 2.9.2.
% Encoding: UTF8

@INCOLLECTION{Freivalds1981,
  author = {Freivalds, Rūsiņš},
  editor = {Gruska, Jozef and Chytil, Michal},
  title = {Probabilistic two-way machines},
  booktitle = {Mathematical Foundations of Computer Science 1981},
  year = {1981},
  volume = {118},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  isbn = {978-3-540-10856-6},
  pages = {33-45},
  doi = {10.1007/3-540-10856-4_72},
  url = {http://dx.doi.org/10.1007/3-540-10856-4_72}
}

@ARTICLE{Bel,
    author = "Richard Bellman",
     title = "A Markovian Decision Process",
   journal = "Indiana Univ. Math. J.",
  fjournal = "Indiana University Mathematics Journal",
    volume = 6,
      year = 1957,
     issue = 4,
     pages = "679--684",
      issn = "0022-2518",
     coden = "IUMJAB",
   mrclass = "",
}

@BOOK{Barto,
  title = "Reinforcement learning: An introduction",
  author = "Barto, Andrew G",
  year = 1998,
  publisher = "MIT press",
}

@article{Hendler1990ai,
  title={AI planning: Systems and techniques},
  author={Hendler, James A and Tate, Austin and Drummond, Mark},
  journal={AI magazine},
  volume={11},
  number={2},
  pages={61},
  year={1990}
}

@incollection{Otterlo,
year={2012},
isbn={978-3-642-27644-6},
booktitle={Reinforcement Learning},
volume={12},
series={Adaptation, Learning, and Optimization},
editor={Wiering, Marco and van Otterlo, Martijn},
doi={10.1007/978-3-642-27645-3_1},
title={Reinforcement Learning and Markov Decision Processes},
url={http://dx.doi.org/10.1007/978-3-642-27645-3_1},
publisher={Springer Berlin Heidelberg},
author={van Otterlo, Martijn and Wiering, Marco},
pages={3-42},
language={English}
}

@article{koenig2002interaction,
  title={The interaction of representations and planning objectives for decision-theoretic planning tasks},
  author={Koenig, Sven and Liu, Yaxin},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  volume={14},
  number={4},
  pages={303--326},
  year={2002},
  publisher={Taylor \& Francis}
}

@inproceedings{Hasselt2007,
  title={Reinforcement learning in continuous action spaces},
  author={Van Hasselt, Hado and Wiering, Marco A},
  booktitle={Approximate Dynamic Programming and Reinforcement Learning, 2007. ADPRL 2007. IEEE International Symposium on},
  pages={272--279},
  year={2007},
  organization={IEEE}
}

@incollection{Hasselt2012,
  title={Reinforcement learning in continuous state and action spaces},
  author={Van Hasselt, Hado},
  booktitle={Reinforcement Learning},
  pages={207--251},
  year={2012},
  publisher={Springer}
}

@article{nguyen2011model,
  title={Model learning for robot control: a survey},
  author={Nguyen-Tuong, Duy and Peters, Jan},
  journal={Cognitive processing},
  volume={12},
  number={4},
  pages={319--340},
  year={2011},
  publisher={Springer}
}

@article{tesauro1995temporal,
  title={Temporal difference learning and TD-Gammon},
  author={Tesauro, Gerald},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995}
}

@article{mnih2013playing,
  title={Playing Atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{srivastava2014dropout,
  title={Dropout: A simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{sarle1995stopped,
  title={Stopped training and other remedies for overfitting},
  author={Sarle, Warren S},
  booktitle={Proceedings of the 27th Symposium on the Interface of Computing Science and Statisfi (.'. \_$\backslash$‘. pp. 352-360. Interface Foundation of North America, Fairfax Station. VA, USA},
  year={1995}
}

@phdthesis{Werbos74,
    address = {Cambridge, MA},
    author = {Werbos, P.},
    school = {Harvard University},
    title = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
    year = {1974}
}

@incollection{Rumelhart1988,
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
 chapter = {Learning Representations by Back-propagating Errors},
 title = {Neurocomputing: Foundations of Research},
 editor = {Anderson, James A. and Rosenfeld, Edward},
 year = {1988},
 isbn = {0-262-01097-6},
 pages = {696--699},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=65669.104451},
 acmid = {104451},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2546--2554},
  year={2011}
}

@inproceedings{bhatnagar2009convergent,
  title={Convergent temporal-difference learning with arbitrary smooth function approximation},
  author={Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S and Maei, Hamid R and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1204--1212},
  year={2009}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation.},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  journal={Advances in Neural Information Processing Systems},
  volume={12},
  pages={1057--1063},
  year={2000},
  publisher={MIT press}
}

@article{halton1970retrospective,
  title={A retrospective and prospective survey of the Monte Carlo method},
  author={Halton, John H},
  journal={Siam review},
  volume={12},
  number={1},
  pages={1--63},
  year={1970},
  publisher={SIAM}
}

@inproceedings{Hasselt2009,
  title={Using continuous action spaces to solve discrete problems},
  author={van Hasselt, Hado and Wiering, Marco A},
  booktitle={Neural Networks, 2009. IJCNN 2009. International Joint Conference on},
  pages={1149--1156},
  year={2009},
  organization={IEEE}
}

@article{Levine2013,
  title={Exploring Deep and Recurrent Architectures for Optimal Control},
  url={http://arxiv.org/abs/1311.1761},
  journal={arXiv preprint arXiv:1311.1761},
  author={Levine, Sergey},
  year={2013},
  pages={1--8}
}

@article{Clark2013,
  abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this "hierarchical prediction machine" approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
  author = {Clark, Andy},
  doi = {10.1017/S0140525X12000477},
  file = {:home/rihards/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clark - 2013 - Whatever next Predictive brains, situated agents, and the future of cognitive science.pdf:pdf},
  issn = {1469-1825},
  journal = {The Behavioral and brain sciences},
  keywords = {Attention,Attention: physiology,Brain,Brain: physiology,Cognition,Cognition: physiology,Cognitive Science,Cognitive Science: trends,Humans,Learning,Learning: physiology,Models,Neurological,Perception,Perception: physiology,neuroscience,predictive},
  mendeley-tags = {neuroscience,predictive},
  month = {jun},
  number = {3},
  pages = {181--204},
  pmid = {23663408},
  title = {{Whatever next? Predictive brains, situated agents, and the future of cognitive science.}},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/23663408},
  volume = {36},
  year = {2013}
}

@unpublished{Krislauks2015,
author = {Kri{\v{s}}lauks, Rihards},
file = {:home/rihards/Documents/Papers/ml/rl/nncasmdp.pdf:pdf},
organization = {Latvijas Universitāte, Datorikas fakultāte},
title = {{Neironu tīkli un nepārtrauktas darbību telpas markova izvēles procesi}},
year = {2015}
}

@article{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Y., Lecun and Y., Bengio and G., Hinton},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/rihards/Documents/Papers/ml/nn/NatureDeepReview.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930630277{\&}partnerID=40{\&}md5=befeefa64ddca265c713cf81f4e2fc54},
volume = {521},
year = {2015}
}

@article{rl-glue,
	Author = {Brian Tanner and Adam White},
	Date-Added = {2009-05-06 22:44:59 -0600},
	Date-Modified = {2009-10-06 13:44:04 -0600},
	Journal = {Journal of Machine Learning Research},
	Month = {September},
	Pages = {2133--2136},
	Title = {{RL}-{G}lue : Language-Independent Software for Reinforcement-Learning Experiments},
	Volume = {10},
	Year = {2009}}

@online{rl-library,
  title = {RL-Library},
  url = {http://library.rl-community.org},
  urldate = {2016-06-05}
}

@MISC{Bastien-Theano-2012,
        author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
      abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.}
}

@INPROCEEDINGS{bergstra+al:2010-scipy,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}

@misc{lasagne,
  title = {Lasagne},
  year = {2014},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Lasagne/Lasagne}}
}

@misc{ipyparallel,
  title = {IPython Parallel},
  year = {2008},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/ipython/ipyparallel}}
}

@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/BF00992698},
file = {:home/rihards/Documents/Papers/ml/rl/q-learning.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125, 1573-0565},
journal = {Machine Learning},
keywords = {-learning,0,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
url = {http://link.springer.com/article/10.1007/BF00992698$\backslash$nhttp://link.springer.com/content/pdf/10.1007{\%}2FBF00992698.pdf},
volume = {8},
year = {1992}
}

@phdthesis{Watkins1989,
author = {Watkins, C. J. C. H},
file = {:home/rihards/Documents/Papers/ml/rl/q-learning-watkins-thesis.pdf:pdf},
school = {King's College},
location = {UK},
title = {{Learning from delayed rewards}},
year = {1989}
}
