\documentclass{ludis} % pieejams https://github.com/rihardsk/LU-nosl-guma-darbs---LaTeX

% xelatex
\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}

%\usepackage[utf8]{inputenc}

\usepackage[]{hyperref}
\hypersetup{
    colorlinks=false
}
\urlstyle{same}

% languages
\usepackage{fixlatvian}
\usepackage{polyglossia}
\setdefaultlanguage{latvian}
\setotherlanguages{english,russian}

% fonts
\usepackage{xltxtra}
% \setmainfont[Mapping=tex-text]{Times New Roman}
%\defaultfontfeatures{Scale=MatchLowercase,Mapping=tex-text}

% bibliography
%\usepackage{csquotes}
\usepackage[
    backend=biber,
    style=numeric-comp,
    sorting=none,
    natbib=true,
    url=false,
    doi=true%,
    %eprint=false
]{biblatex}
\addbibresource{bibliography.bib}

% toc
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

%tables
\usepackage{longtable}

%papildus matemātika
\usepackage[showonlyrefs]{mathtools}
\newenvironment{thmenum}
 {\begin{enumerate}[label=\upshape(\arabic*),ref=\thethm(\arabic*)]}
 {\end{enumerate}}
\usepackage{amsmath}

%pseidokodam
%\usepackage[noend]{algpseudocode}
%\usepackage{algpseudocode}
\usepackage[boxed,linesnumbered]{algorithm2e}
\SetAlgorithmName{Algoritms}{}{Algoritmu saraksts}

%main line spacing, kur vajag
\usepackage{setspace}

%lai flushotu floatus
\usepackage{placeins}

%images
\usepackage{graphicx}
\usepackage{float}

%dalīšana kolonnās
\usepackage{multicol}

%saraksti
\usepackage{enumitem}

\fakultate{Datorikas}
\nosaukums{Paredzošā stimulētā mācišanās}
\darbaveids{Maģistra kursa}
\autors{Rihards Krišlauks}
\studapl{rk09006}
\vaditajs{Asoc.prof., Dr. dat. Jānis Zuters}
%\recenzents{Juris Vīksna profesors Dr.sc.comp.}
\vieta{Rīga}
\gads{2015}

\begin{document}
\maketitle

\begin{abstract-lv}
  TODO

\keywords{stimulētā mācīšanās; neironu tīkli; Markova izvēles procesi; nepārtrauktas telpas.}
\end{abstract-lv}
\clearpage

\begin{abstract-en}
  TODO

\keywords{reinforcement learning; artificial neural networks; Markov decision processes; continuous spaces.}
\end{abstract-en}


\tableofcontents

\iffalse
\specnodala{Apzīmējumu saraksts}
\setlength\LTleft{0pt}
\setlength\LTright{0pt}
\begin{longtable}{| c | p{28em} |}
  \hline
  \textbf{Apzīmējums} & \textbf{Atšifrējums}\\ 
  \endhead

  \hline
  $D_X \in \mathbb{N}_+$ & \\ %TODO šis jādefinē arī telpas elementiem
  $S \subseteq \mathbb{R}^{D_S}$ & \\
  $A \subseteq \mathbb{R}^{D_A}$ & \\
  $R:S \times A \times S \rightarrow \mathbb{R}$ & \\
  $T:S \times A \times S \rightarrow [0,1]$ & Apzīmējuma nosaukums \\
  $\pi(s, a)$ &  Apzīmējuma nosaukums 2\\
  \hline
\end{longtable}
\fi

\specnodala{Ievads}
Pēdējā laikā teorētiskajā neirozinātnē plašu piekrišanu sāk iegūt t.s.
paredzošās kodēšanas (predictive coding) teorija, kuras pamattēze ir --
smadzenes ir paredzēšanas mašīnas, tās pastāvīgi cenšas sapārot ienākošos
sensoru signālus ar augsta līmeņa paredzējumiem ar mērķi minimizēt paredzēšanas
kļūdas. Paredzošā kodēšana sniedz vienotu skatījumu uz procesiem, kas ir pamatā
cilvēka apziņai, un piedāvā mehānismu, kas ļauj izskaidrot plašu klāstu ar
neirozinātnē pētītiem cilvēka apziņas fenomeniem \autocite{Clark2013}.

Ideja par smadzenēm kā māšīnām, kas, ņemot vērā pašreizējos novērojumus, cenšas
paredzēt nākotnes novērojumus, ir ļoti pievilcīga no dažādu mašīnmācīšanās
paradigmu skatpunkta. Šis vienkāršais mehānisms šķiet viegli formulējams kā
mašīnmācīšanās problēma, un šķiet it īpaši piemērots, lai to izteiktu kā
stimulētās mācīšanās problēmu.

Stimulētā mācīšanās ir mašīnmācīšanās paradigma, kas formalizē mācīšanos kā
mašīnmācīšanās uzdevumu. Tās pamatā ir ideja par vidi un aģentu, kas tajā
darbojas. Aģenta mērķis ir, ņemot vērā ārēju atalgojuma signālu, veikt darbības
vidē tā, lai maksimizētu saņemto atalgojumu. Šis vienkāršais, bet spēcīgais,
uzstādījums ļauj aprakstīt un risināt ļoti plašu problēmu loku, kur optimālā
rīcības stratēģija nav zināma, bet ir iespējams nodefinēt stāvokli, ko aģentam
būtu jācešas sasniegt. Aģenta ziņā paliek, darbojoties vidē, laika gaitā atrast
optimālo stratēģiju, kas ļautu sasniegt pēc iespējas lielāku atalgojumu. Kā
piemērus šādi risināmām problēmām var minēt orientēšanos labirintā vai
automātisku lidaparāta kontroli.

No augstāk minētā dabīgi seko doma par paredzošās kodēšanas ideju pielietošanu
stimulētās mācīšanās uzdevumu risināšanā. Interesi raisa ideja par vides nākamā
stāvokļa paredzēšanu, ņēmot vērā pašreizējo, un vai tas palīdzētu mācīšanās
procesā. Tas liktu stimulētās mācīšanās aģentam tuvināti iemācīties vides
modeli. Tuvākas izpētes vērts ir jautājums, vai šāds iekšējs modelis palīdzētu
macīšanās procesā.

Šajā darbā tiks pētītas iespējas, izmantot paredzošās kodēšanas idejas stimulētās
mācīšanas uzdevumu risināšanai, kā arī tiek pētītas dažādas citas iespējas ar
papildus informāciju uzlabot mācīšanās ātrumu. Darbs tiek iesākts ar vispārīgu stimulētās
mācīšanās paradigmas apskatu, kur autors galvenokārt pieturas pie izklāsta kas
atrodams \autocite{Krislauks2015}. Tam seko COMBO CACLA algoritma apraksts, un
salīdzinājums ar CACLA algoritmu, uz kā tas ir balstīts. %TODO papildināt, kad
                                %ir vairāk gatavs no satura
%NOTE: pie cacla algoritma jāpastāsta, kas ir tie aspekti, kas padara viņu
%pievilcīgu modifikācijām.
\chapter{CCACLA algoritms}\label{chap:ccacla}
CACLA algoritms izmanto divus neironu tīklus, no kuriem viens aproksimē optimālo
stratēģiju, bet otrs -- stāvokļu vērtības funkciju. Abu neironu tīklu parametri
tiek pielāgoti, tiem savstarpēji mijiedarbojoties algoritma darbības laikā --
stāvokļu vērtības funkcija ir atkarīga no optimālās stratēģijas aproksimācijas,
jo tā nosaka, kādas darbības tiek veiktas katrā stāvoklī, kas laika gaitā
ietekmē stāvokļu vērtību, savukārt optimālās stratēģijas aproksimācija tiek
pielāgota atkarībā no stāvokļu funkcijas izmaiņām, ņemot vērā, vai stratēģijas
noteiktā darbība dotā stāvoklī pēc izpildes palielina stāvokļa vērtību. Šīs
savstarpējās mijiedarbības rezultātā algritms ir spējīgs mācīties, bet rodas
jautājums, vai šādi netiek darīts lieks darbs. Gan stāvokļu vērtības funkcijas
aproksimācijas $V$ gan optimālās stratēģijas funkcijas aproksimācijas $A$ domēns
ir vides stāvokļu telpa $S$. Ir saprātīgi pieņemt, ka abu funkciju vērtības
varētu noteikt līdzīgas pazīmes stāvokļu telpā. Abu attiecīgo neironu tīklu
otrais slānis varētu būt trenēšānas rezultātā iemācījies izejā dot līdzīgus
aktivācijas vienos un tajos pašos stāvokļus, tāpēc rodas jautājums, vai abus
tīklus nav iespējams apvienot.

Skatījumu uz neironu tīkliem kā hierarhiskiem pazīmju izgūšanas rīkiem sniedz
pēdējā laikā lielu popularitāti ieguvusī dziļās mācīšanās paradigma. Tās pamatā
ir ideja, ka daudzos dažādos ar reālo pasauli saistītos domēnos, piemēram, attēlu
vai runas atpazīšānā, dabīgās valodas apstrādē u.c., pētāmos objektus ir
iespējams raksturot ar hierarhisku vairāklīmeņu pazīmju kopumu \autocite{Lecun2015}. Tas ļauj
izmantot vairākļīmeņu neironu tīklus šo pazīmju izgūšanai. %TODO piemērs
Lai arī šajā darbā apskatītajās stimulētās mačīšanās problēmas atāvokļu telpas
nav tik sarežģītas, lai būtu nepieciešamība izmanot dziļus neironu tīklus, tomēr
arī uz sekliem neironu tīkliem var raudzīties kā uz ierīcēm, kas izgūst zema
līmeņa pazīmes, ko pēdējais tīkla līmenis ir iemācījies apstrādāt. Šis skatījums
rada papildus motivāciju pētīt, vai nav iespējams lietderīgi izmantot CACLA
algoritmā izmantoto neironu tīklu apakšējo līmeņu izgūto pazīmju īpatnības.

Šajā nodaļā tiks apskatītas vairākas CACLA algoritma modifikācijas, kuras to
kopīgo īpašību dēļ autors ievieto vienā algoritmu grupā ar kopīgo nosaukumu
Combined CACLA, jeb CCACLA. Visu algoritmu kopīgā iezīme, kas tos atšķir no
iepriekš apskatītā CACLA algoritma ir viena neironu tīkla izmantošana gan
vērtību funkcijas $V$ gan optimālās stratēģijas funkcijas $A$ aproksimācijai.
Tiek apskatīti vairāki šī algoritma paveidi, kas ietver modifikācijas tajā, kāda
informācija tiek izmantota tīkla trenēšanai.

\section{CCACLA pamatalgoritms}
CCACLA vienkāršākais variants, kas vismazāk atšķiras no par pamatu ņemtā CACLA
algoritma, izmanto funkciju 
\begin{equation}
  Av_\Theta:S \rightarrow A \times \mathbb{R},
\end{equation}
lai stāvoklī $s$ paredzētu gan optimālo darbību $a$, gan stāvokļa vērtību $v$ kā
$Av_\Theta(s) = (a, v)$. Funkcija $Av_\Theta$ tiek realizēta ar
neironu tīklu, kas sastāv no ieejas slāņa, slēptā slāņa, un diviem izejas
slāņiem, kas slēptā slāņa aktivācijas pārvērš, respektīvi, optimālajā darbībā
$a$ un stāvokļa vērtībā. Šīs vērtības tad attiecīgi var izteikt, kā
\begin{equation}\label{eq:ccacla-act}
  v = V(s), V(s) = f_a(f_h(s^T \cdot W_h) \cdot W_a)
\end{equation} un
\begin{equation}\label{eq:ccacla-val}
  a = Ac(s), Ac(s) = f_v(f_h(s^T \cdot W_h) \cdot W_v)
\end{equation}, kur $s$ ir
ieejas vektors (stāvoklis), $W_h$ ir slēptā slāņa svaru matrica, $W_a$ ir
darbības slāņā svaru matrica, $W_v$ ir vērtības slāņa svaru matrica, $f_h$ ir
slēptā slāņā aktivācijas funkcija, $f_a$ ir darbības slāņā aktivācijas funkcija
un $f_v$ ir vērtības slāņā aktivācijas funkcija. Ar ieviestajiem mainīgajiem
varam arī izteikt funkciju $Av$
\begin{equation}\label{eq:ccacla-actval}
  Av(s) = (f_a(h^{(1 \ldots D(A))}), f_v(h^{(D(A) + 1)})), \text{ kur } 
  h = f_h(f_h(s^T \cdot W_h) \cdot W_v)
\end{equation}
%TODO: pateikt, ka bias vērtības ir iekļautas W

Citos aspektos CCACLA pamatalgoritms neatšķiras no CACLA. Šādi izteiktas funkcijas
$V(s)$ un $Ac(s)$ ļauj izmantot jau iepriekš definēto CACLA algoritmu <reference
uz cacla pseidokodu>. Atbilstoši vienādojumam \ref{eq:ccacla-actval} iespējams
arī definēt optimizētu versiju, kas izmanto faktu, ka $a$ un $v$ iespējams
izrēķināt vienlaicīgi. Tas parādīts algoritmā \ref{alg:ccacla-basic}.

\begin{spacing}{1}
\begin{algorithm}
\caption{CACLA pseidokods}\label{alg:ccacla-basic}
inicializē $\theta_0, \psi_0, s_0$ \\
$(a_0', v_0) := Av_0(s_0)$ \\
ņem $a_0$ ar izkliedi ap $a_0'$ \\
\For{$t \in \{1,2,\ldots\}$}{
	veic $a_{t-1}$, novēro $r_{t-1}, s_t$ \\
  $(a_t', v_t) := Av_t(s_t)$ \\
	ņem $a_t$ ar izkliedi ap $a_t'$ \\
	\eIf{$s_t$ ir gala stāvoklis}{
		$V_t(s_{t-1}) \xleftarrow{\alpha_{t-1}(s_{t-1})} r_{t-1}$ \\
		ņem jaunu $s_t$ ($s_t$ ir sākumstāvoklis nākamajā trenēšanas epizodē)
    % TODO: šeit jāiestata arī a un v (vai arī jāpievieno ārējais cikls, un šeit
    % var rakstīt goto 1)
	}{
		$V_t(s_{t-1}) \xleftarrow{\alpha_t(s_{t-1})} r_{t-1} + \gamma v_t$
	}
	\If{$r_{t-1} + \gamma v_t > v_{t-1}$}{
		$V_t(s_{t-1}) \xleftarrow{\beta_t(s_{t-1})} a_{t-1}$
	}
}
\end{algorithm}
\end{spacing}

% Algoritma \ref{alg:ccacla-basic} 9. un 14. rindiņā nepieciešams veikt gradient
% descent soļus funkcijām \ref{eq:ccacla-act} un \ref{eq:ccacla-val}, kam
% nepieciešami šo funkciju diferenciāļi.
% \begin{equation}
%   \frac{\partial V}{\partial s} = 
% \end{equation}
%TODO varbūt jāpastāsta par bakcprop. skat: https://en.wikipedia.org/wiki/Backpropagation

\section{CCACLA ar stāvokļa paredzēšanu}
Kā tika minēts iepriekš, interesi rada jautājums, vai ir iespējams ar kādas
papildinformācijas palīdzību trenēšanas laikā likt stimulētās mācīšanās aģentam
veidot iekšēju vides modeli ar mērķi paātrināt mācīšanos. Šajā nolūkā tiek
ieviests CCACLA algoritms ar stāvokļa paredzēšanu, kas no iepriekš aprakstītā
CCACLA pamatalgoritma atšķiras ar to, ka kombinētajam neironu tīklam tagad ir
jāparedz arī nākamais stāvoklis, kurā nonāks aģents pēc paredzētās optimālās
darbības veikšanas. Tas noved pie funkcijas $Aov$ 
\begin{equation}
  Aov_\Theta:S \rightarrow A \times S \times \mathbb{R}
\end{equation}
Līdz ar funkciju \ref{eq:ccacla-act} un \ref{eq:ccacla-val} tagad ieviesīsim arī
funkciju $St$, kas vēlāk tiks izmantota, CCACLA' algiritma pseidokodā, lai
atvieglotu gradient descent soļu pierakstu
\begin{equation}\label{eq:ccacla-obs}
  s' = St(s), St(s) = f_s(f_h(s^T \cdot W_h) \cdot W_a)
\end{equation} 
, kur $W_S$ ir nākamā stāvokļa paredzēšanas slāņa svaru matrica, un $f_S$ ir
slāņa aktivācijas funkcija.
Analogi vienādojumam \ref{eq:ccacla-actval} tagad varam definēt funkciju $Aov$,
kas parāda, kā funkciju \ref{eq:ccacla-act}, \ref{eq:ccacla-act} un
\ref{eq:ccacla-act} apvieno vienā neironu tīklā.
\begin{equation}\label{eq:ccacla-actobsval}
  Aov(s) = (f_a(h^{(1 \ldots D(A))}),
                 f_s(h^{(D(A) + 1 \ldots D(A) + D(S))}),
                 f_v(h^{(D(A) + D(S) + 1)}))
  \text{ kur } h = f_h(f_h(s^T \cdot W_h) \cdot W_v)
\end{equation}

Lai aprakstītu CACLA' pseidokodā pietrūkst vēl viena funkcija. Atšķirība no
CCACLA pamatalgoritma ir tajā, ka tagad nākas šķirot gadījumus, kuros
trenēšanai kā ieejas dati ir pieejama stāvokļa vārtība, kuros nākamais
stāvoklis, kuros optimālā darbība, un kuros ir pieejama kāda kombinācija no
minētajām vērtībām. Tādēļ nākas ieviest funkciju, nākamā stāvokļa paredzēšānai
un esošā stāvokļa vērtības paredzēšanai.
\begin{equation}
  Ov: S \rightarrow S \times \mathbb{R},
        Ov(s) = (f_s(h^{(D(A) + 1 \ldots D(A) + D(S))}),
                 f_v(h^{(D(A) + D(S) + 1)}))
  \text{ kur } h = f_h(f_h(s^T \cdot W_h) \cdot W_v)
\end{equation}
Ar tās un iepriekš definēto funkciju paldīzību varam ideju definēt precīzāk
algoritmā \ref{alg:ccacla-state}.

\begin{spacing}{1}
\begin{algorithm}
\caption{CACLA pseidokods}\label{alg:ccacla-state}
inicializē $\theta_0, \psi_0, s_0$ \\
$(a_0', v_0) := Av_0(s_0)$ \\
ņem $a_0$ ar izkliedi ap $a_0'$ \\
\For{$t \in \{1,2,\ldots\}$}{
	veic $a_{t-1}$, novēro $r_{t-1}, s_t$ \\
  $(a_t', v_t) := Av_t(s_t)$ \\
	ņem $a_t$ ar izkliedi ap $a_t'$ \\
	\eIf{$s_t$ ir gala stāvoklis}{
    \eIf{$r_{t-1} + \gamma v_t > v_{t-1}$}{
      $Av_t(s_{t-1}) \xleftarrow{\alpha_{t-1}(s_{t-1})} (a_{t-1}, r_{t-1})$ \\
    }{
      $V_t(s_{t-1}) \xleftarrow{\alpha_{t-1}(s_{t-1})} r_{t-1}$ \\
    }
		ņem jaunu $s_t$ ($s_t$ ir sākumstāvoklis nākamajā trenēšan as epizodē)
    % TODO: šeit jāiestata arī a un v (vai arī jāpievieno ārējais cikls, un šeit
    % var rakstīt goto 1)
	}{
    \eIf{$r_{t-1} + \gamma v_t > v_{t-1}$}{
      $Aov_t(s_{t-1}) \xleftarrow{\alpha_t(s_{t-1})} (a_{t-1}, s_t, r_{t-1} + \gamma v_t)$
    }{
      $Ov_t(s_{t-1}) \xleftarrow{\alpha_t(s_{t-1})} (s_t, r_{t-1} + \gamma v_t)$
    }
	}
	\If{$r_{t-1} + \gamma v_t > v_{t-1}$}{
		$V_t(s_{t-1}) \xleftarrow{\beta_t(s_{t-1})} a_{t-1}$
	}
}
\end{algorithm}
\end{spacing}

\section{CCACLA ar paredzēšanu nākotnē}
$Aov'_\Theta:S \rightarrow A \times S \times \mathbb{R}$
 
\chapter{Diskusija}
TODO
\chapter{Secinājumi}
TODO


\printbibliography
\end{document}

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% reftex-default-bibliography: ("bibliography.bib")
%%% End:

%%% TeX-command-extra-options: "-shell-escape"